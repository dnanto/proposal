# Proposal {.unnumbered}

## Introduction {.unnumbered}

This research relies on molecular clock methods to estimate substitution rates and divergence times in *Human adenovirus* samples. This requires a highly curated data set of time-stamped, homologous, nucleic acid sequences representative of a measurably evolving population [@drummondMeasurablyEvolvingPopulations2003]. Thus, a preprocessing workflow is necessary to extract and normalize sampling date information. The presence of heterogeneous or ambiguous date formats is a complicating factor and a time sink. Also, manual preprocessing becomes intractable as the number of samples increases. As a result, this research defines a generic workflow and automated, parallel pipeline that coordinates the execution of preprocessing and analytical tasks in a reproducible manner.

## Methods {.unnumbered}

This section documents an executable pipeline that constructs data sets for molecular clock analysis. The solution relies on Snakemake, which is a portable, rule-based workflow engine [@kosterSnakemakeScalableBioinformatics2012]. Each rule defines a step in the workflow. A rule can also configure its execution environment via the Conda package manager. The engine automatically infers the workflow path and parallelization based on input dependencies, creating a directed acyclic graph [@kosterSnakemakeScalableBioinformatics2012]. Accordingly, the engine guarantees the reproducibility of each step.

### Phase 1: Data Set Generation {.unnumbered}

The first phase uses a query to generate a set of timestamped, homologous sequences. The execution has two initial paths to process genes and genomes separately. Both paths use different strategies to guarantee full alignment coverage of the query. For genes, the first rule runs the `BLAST+` `blastn` program to perform local alignment and generate a library of sequences. The next rule runs the `FASTA` `glsearch36` program to perform global-local alignment. For genomes, the first rule runs the BLAST+ `blastdbcmd` program to subset the BLAST database by sequence length, accepting those within a percentage deviation of expected size. The next rule runs the `nucmer` and `show-coords` programs of the `MUMmer` genome alignment suite. Each path generates a report of query coverage identity scores.

The next rule defines a Python program that extracts the accessions from the report. It uses an Entrez Direct binding of the `esummary` utility to query GenBank and download a JSON file of the metadata. The final rule processes the JSON file and query coverage identity report. It extracts the "collection_date" qualifier and attempts to normalize it into an ISO-8601 string from a list of formats. The rule then accepts sequences based on an identity threshold and successful date extraction.

### Phase 2: Phylogenetic Analyses {.unnumbered}

The next phase performs phylogenetic analyses on the generated data set. The initial rule runs the `mafft` program to generate a multiple sequence alignment. This program calculates a fast Fourier transform to cluster and progressively align the sequences [@katohMAFFTNovelMethod2002a]. It also automatically sets the optimal program execution mode based on input size and reverse complements any sequence if necessary [@katohMAFFTNovelMethod2002a]. Downstream rules process the resulting FASTA file.

Variant detection occurs in a rule that runs the `snp-sites` program. This program builds a consensus sequence off of the multiple alignment and outputs SNPs and indels accordingly [@pageSNPsitesRapidEfficient2016]. The resulting VCF file stores the call and position for each entry in the alignment.

Tree inference occurs in a rule that runs the `iqtree` program to infer a maximum-likelihood tree [@nguyenIQTREEFastEffective2015a]. The program uses `ModelFinder` to calculate the best substitution model based on the Bayesian information criterion [@kalyaanamoorthyModelFinderFastModel2017a]. The "-alrt" and "-bb" flags set the number of bootstrap replicates for the approximate likelihood ratio test of branches and branch support and the "-bnni" flag activates nearest neighbor interchange search optimization strategy [@anisimovaSurveyBranchSupport2011a; @hoangUFBoot2ImprovingUltrafast2018a]. The program outputs a log file and exports the tree in Newick format.

Recombination analysis occurs in a rule that runs the `ClonalFrameML` program. It improves upon the original `ClonalFrame` program by using maximum-likelihood estimation to identify recombinant regions based on the multiple sequence alignment and original tree [@didelotClonalFrameMLEfficientInference2015a; @didelotInferenceBacterialMicroevolution2007]. This step is necessary since recombination obscures temporal analysis [@didelotClonalFrameMLEfficientInference2015a; @rambautExploringTemporalStructure2016a]. Accordingly, the output is a tree based on the clonal frame, the sequence with recombinant regions removed. [@milkmanMolecularEvolutionEscherichia1990]. 

Estimation of evolutionary rate and ancestral dates occurs in a rule that uses the `BactDating` R package. The `bactdate` function performs a Markov chain Monte Carlo simulation to estimate parameters for the strict and relaxed clock models [@didelotBayesianInferenceAncestral2018a]. This method considers the `ClonalFrameML` tree to correct for branch lengths affected by recombination [@didelotBayesianInferenceAncestral2018a]. The rule outputs a serialized R data object for downstream analysis.

The pipeline also includes a rule to output BEAST input files. The rule executes a custom Python script that transforms the multiple sequence alignment and `ModelTest` result into an XML file. The file specifies the sequence and sampling date for each taxon. It also maps the inferred substitution model to a BEAST compatible format. Also, the script includes specifications to run the maximum likelihood estimation using the path sampling stepping stone method. The rule exports the XML file for the user to run it on a computational cluster since due to the computational expense. The following section executes this workflow to validate results from the pipeline describes in this section. 


### Validation {.unnumbered}

...

## Results {.unnumbered}

...

## References {.unnumbered}
